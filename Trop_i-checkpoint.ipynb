{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8286de47-6ffd-4de6-a692-857297c852dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bottleneck in c:\\users\\wishes lawrence\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\wishes lawrence\\anaconda3\\lib\\site-packages (from bottleneck) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8a558d-12d8-4054-9993-4f37b550bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-notifications\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (optional)\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "website='https://dir.indiamart.com/impcat/troponin-i-test-kit.html?biz=30'\n",
    "#website = 'https://dir.indiamart.com/impcat/troponin-i-test-kit.html?biz=10'\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(website)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2540abc-1da9-4de8-9b18-95894ead6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random delay to prevent bot detection\n",
    "time.sleep(random.randint(5, 10))\n",
    "container=webdrive\n",
    "# Check if page opened successfully\n",
    "print(f\"Scraping started: {driver.title}\")\n",
    "\n",
    "# Lists to store extracted data\n",
    "product_name = []\n",
    "product_price = []\n",
    "product_ratings = []\n",
    "product_ratings_num = []\n",
    "product_bought = []\n",
    "\n",
    "# Pagination loop\n",
    "page_number = 1\n",
    "max_pages = 53 # Change this to 160 if needed\n",
    "\n",
    "while page_number <= max_pages:\n",
    "    try:\n",
    "        print(f\"Scraping page {page_number}: {driver.current_url}\")\n",
    "\n",
    "        # Scroll down to load all elements\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(random.uniform(3, 6))\n",
    "\n",
    "        # Wait for product items to load\n",
    "        items = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH,'//li[contains(@class, \"lst lst_cl    mft2 img slc\")]')))\n",
    "\n",
    "        for item in items:\n",
    "            try:\n",
    "                name = item.find_element(By.XPATH, '//li[contains(@class, \"lst lst_cl    mft2 img slc\")]//span[contains(@class,\"lg elps\")]//h3').text\n",
    "                product_name.append(name)\n",
    "            except NoSuchElementException:\n",
    "                product_name.append(\"N/A\")\n",
    "\n",
    "            try:\n",
    "                price = item.find_element(By.XPATH, '//*[@id=\"1prcenq\"]/span/text()')\n",
    "                product_price.append(price)\n",
    "            except NoSuchElementException:\n",
    "                product_price.append(\"N/A\")\n",
    "\n",
    "            try:\n",
    "                form = item.find_element(By.XPATH, '//*[@id=\"LST1\"]/div/div[1]/div[2]/div/div/table/tbody/tr[1]/td[2]/div/span').text\n",
    "                product_ratings.append(ratings)\n",
    "            except NoSuchElementException:\n",
    "                product_ratings.append(\"N/A\")\n",
    "\n",
    "            try:\n",
    "               brand = item.find_element(By.XPATH, '//*[@id=\"LST1\"]/div/div[1]/div[2]/div/div/table/tbody/tr[3]/td[2]/div/span').text\n",
    "                product_ratings_num.append(ratings_num)\n",
    "            except NoSuchElementException:\n",
    "                product_ratings_num.append(\"N/A\")\n",
    "\n",
    "            try:\n",
    "                no_kits = item.find_element(By.XPATH, '//*[@id=\"LST1\"]/div/div[1]/div[2]/div/div/table/tbody/tr[5]/td[2]/div/span').text\n",
    "                product_bought.append(bought)\n",
    "            except NoSuchElementException:\n",
    "                product_bought.append(\"N/A\")\n",
    "\n",
    "        print(f\"Page {page_number} scraped. Extracted {len(items)} items.\")\n",
    "\n",
    "        # Check for \"Next\" button\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//a[contains(@class, \"s-pagination-next\")]'))\n",
    "            )\n",
    "            next_button.click()\n",
    "            page_number += 1\n",
    "            time.sleep(random.uniform(5, 10))  # Allow time for next page to load\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            print(\"No more pages found or failed to click Next.\")\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on page {page_number}: {e}\")\n",
    "        break\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'product_name': product_name,\n",
    "    'product_price': product_price,\n",
    "    'product_ratings': product_ratings,\n",
    "    'product_ratings_num': product_ratings_num,\n",
    "    'product_bought': product_bought\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('amazon_india_chemical_resistant_gloves.xlsx', index=False)\n",
    "\n",
    "# Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "print(\"Scraping completed and data saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
